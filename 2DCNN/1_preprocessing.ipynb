{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this file the data is preprocessed in order to be readable for the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base libraries\n",
    "import os\n",
    "import csv\n",
    "\n",
    "#Important mathematical libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Wavelet transformation libraries\n",
    "from ssqueezepy import cwt, icwt\n",
    "\n",
    "#Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Other\n",
    "# tqdm makes loading bar\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding anomalies in one column\n",
    "def find_column_anomaly(df, column):\n",
    "    column_anomaly_upper = df[column] > df[column].mean()*1.2\n",
    "    column_anomaly_lower = df[column] < df[column].mean()*0.8\n",
    "    column_anomaly = column_anomaly_upper | column_anomaly_lower\n",
    "    return column_anomaly.values\n",
    "\n",
    "# Finding anomalies in multiple columns\n",
    "def find_anomalies_std(df, columns):\n",
    "    anomalies = []\n",
    "    for column in columns:\n",
    "        anomalies_array = find_column_anomaly(df, column)\n",
    "        anomalies.append(anomalies_array)\n",
    "    is_anomaly = [sum(i)/len(columns) for i in zip(*anomalies)]\n",
    "    df[\"Anomaly\"] = is_anomaly\n",
    "    df[\"Anomaly\"] = df[\"Anomaly\"]> 0.7\n",
    "    return df\n",
    "\n",
    "# Creating frames\n",
    "def get_sets(df, columns, step, row_length):\n",
    "    rows_num = int(len(df)/step - (row_length/step-1))\n",
    "    \n",
    "    values = df[columns].values\n",
    "    anomalies = df['Anomaly'].values\n",
    " \n",
    "    X = np.zeros((rows_num, row_length,len(columns)))\n",
    "    Y = np.zeros((rows_num, 1))\n",
    "    \n",
    "    for i in range(0, rows_num):\n",
    "        first_element = step*i\n",
    "        last_element = step*i+row_length\n",
    "        X[i] = values[first_element:last_element]\n",
    "        Y[i] = anomalies[first_element:last_element].sum()\n",
    "    Y = np.where(Y > 0, 1, 0)\n",
    "    return X, Y\n",
    "\n",
    "# Creating wavelets\n",
    "def wavelet_transformation(X, columns):\n",
    "    X_shape  = cwt(X[0][:,0], 'morlet')[0].shape\n",
    "    data = np.zeros((len(X), X_shape[0], X_shape[1], len(columns)*2))\n",
    "    counter = 0\n",
    "    for row in tqdm(X, desc=f\"Data creation progress...\"):\n",
    "        images = []\n",
    "        for i in range(0, len(row[0])):\n",
    "            Wx, scales = cwt(row[:, i], 'morlet')\n",
    "            real = np.reshape(Wx.real, (Wx.shape[0], Wx.shape[1], 1))\n",
    "            imag = np.reshape(Wx.imag, (Wx.shape[0], Wx.shape[1], 1))\n",
    "            image = np.concatenate([real, imag], axis=2)\n",
    "            images.append(image)\n",
    "        data[counter] = np.concatenate(images, axis=2)\n",
    "        counter +=1\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create directory if does not exist\n",
    "def create_directory(dataset_name):\n",
    "    dir_path = f'datasets/{dataset_name}/'\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading CSV\n",
    "\n",
    "- Reads parameters directly into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thermal values\n",
    "df = pd.read_csv('../data/thm/processed_thermal_data.csv')\n",
    "\n",
    "# OR\n",
    "\n",
    "#EPS values\n",
    "#df = pd.read_csv('../data/eps/processed_test_power_measurements_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading existing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) created_on\n",
      "2) MB_temp\n",
      "3) BMB_temp\n",
      "4) B1_temp\n",
      "5) B2_temp\n",
      "6) B1_temp_adj\n"
     ]
    }
   ],
   "source": [
    "columns_list = list(df.columns)\n",
    "for i, column in enumerate(columns_list[1:]):\n",
    "    print(f\"{i+1}) {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting variables\n",
    "\n",
    "- __frame_size__ is the size of the data chunks, that the data is split into\n",
    "- __step_size__ is the difference in time steps between the start of two consecutive frames\n",
    "- __test_size__ is relation of data points used for training to total number of data points\n",
    "- __columns__ is the names of columns that should be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 20\n",
    "row_length = 40\n",
    "test_size = 0.6\n",
    "columns = [\"B1_temp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data creation progress...: 100%|██████████| 4999/4999 [00:41<00:00, 119.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Limiting the size of the data set\n",
    "df_new = df.iloc[-100000:,:]\n",
    "\n",
    "# Spliting the data into predefined data chunks\n",
    "X, Y =  get_sets(df_new, columns, step, row_length)\n",
    "\n",
    "# Creating a wavelet transform of every data chunk\n",
    "data = wavelet_transformation(X, columns)\n",
    "\n",
    "# Creating training and testing data sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, Y, test_size=test_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save done\n"
     ]
    }
   ],
   "source": [
    "# Naming the directory\n",
    "dataset_name = f'{\"&\".join(columns)}-{row_length}L{step}S{int(test_size*100)}T'\n",
    "\n",
    "# Creating directory if exists\n",
    "create_directory(dataset_name)\n",
    "\n",
    "# Saving\n",
    "np.save(f'datasets/{dataset_name}/X.npy', X)\n",
    "np.save(f'datasets/{dataset_name}/original_data.npy', df_new[columns+[\"Anomaly\"]].values)\n",
    "np.save(f'datasets/{dataset_name}/x_train.npy', x_train)\n",
    "np.save(f'datasets/{dataset_name}/y_train.npy', y_train)\n",
    "np.save(f'datasets/{dataset_name}/x_test.npy', x_test)\n",
    "np.save(f'datasets/{dataset_name}/y_test.npy', y_test)\n",
    "\n",
    "# Log\n",
    "print(\"Save done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
